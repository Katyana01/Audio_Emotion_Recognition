{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amPMF9EU59HO",
        "outputId": "71058125-6786-4c38-b746-99ec12c6d0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "avi_path = \"/content/drive/MyDrive/avi\"\n",
        "if os.path.exists(avi_path):\n",
        "    entries = os.listdir(avi_path)\n",
        "    print(\"Entries in avi:\", entries)\n",
        "else:\n",
        "    print(f\"Path does not exist: {avi_path}\")"
      ],
      "metadata": {
        "id": "5tlC_gJsb-O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive"
      ],
      "metadata": {
        "id": "D7IXENAaccGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get update && apt-get install -y ffmpeg\n",
        "\n",
        "mkdir -p /content/audio\n",
        "\n",
        "for vid in \"/content/drive/My Drive/avi\"/*.avi; do\n",
        "  base=$(basename \"$vid\" .avi)\n",
        "  ffmpeg -i \"$vid\" \\\n",
        "    -vn \\\n",
        "    -ar 16000 \\\n",
        "    -ac 1 \\\n",
        "    -loglevel error \\\n",
        "    \"/content/audio/${base}.wav\"\n",
        "done\n",
        "\n",
        "ls -lh /content/audio | head -n 10\n"
      ],
      "metadata": {
        "id": "7yeK4Pzwct1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "avi_dir = \"/content/drive/My Drive/avi\"\n",
        "wav_dir = \"/content/audio\"\n",
        "\n",
        "avi_files = glob.glob(os.path.join(avi_dir, \"*.avi\"))\n",
        "wav_files = glob.glob(os.path.join(wav_dir, \"*.wav\"))\n",
        "\n",
        "total = len(avi_files)\n",
        "converted = len(wav_files)\n",
        "remaining = total - converted\n",
        "\n",
        "print(f\"Всего видео-файлов:       {total}\")\n",
        "print(f\"Уже конвертировано в WAV: {converted}\")\n",
        "print(f\"Осталось конвертировать:  {remaining}\")\n"
      ],
      "metadata": {
        "id": "ddpCmRWLjzf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tqdm\n",
        "\n",
        "import os, glob, subprocess\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "AVI_DIR = \"/content/drive/My Drive/avi\"\n",
        "WAV_DIR = \"/content/audio\"\n",
        "os.makedirs(WAV_DIR, exist_ok=True)\n",
        "\n",
        "avi_files = glob.glob(os.path.join(AVI_DIR, \"*.avi\"))\n",
        "wav_bases = {\n",
        "    os.path.splitext(os.path.basename(p))[0]\n",
        "    for p in glob.glob(os.path.join(WAV_DIR, \"*.wav\"))\n",
        "}\n",
        "to_do = [\n",
        "    p for p in avi_files\n",
        "    if os.path.splitext(os.path.basename(p))[0] not in wav_bases\n",
        "]\n",
        "total = len(to_do)\n",
        "print(f\"Осталось файлов: {total} из {len(avi_files)}\")\n",
        "\n",
        "def convert_one(vid_path):\n",
        "    base = os.path.splitext(os.path.basename(vid_path))[0]\n",
        "    out_wav = os.path.join(WAV_DIR, f\"{base}.wav\")\n",
        "    cmd = [\n",
        "        \"ffmpeg\", \"-y\", \"-i\", vid_path,\n",
        "        \"-vn\", \"-ar\", \"16000\", \"-ac\", \"1\",\n",
        "        \"-threads\", \"1\", \"-loglevel\", \"error\",\n",
        "        out_wav\n",
        "    ]\n",
        "    subprocess.run(cmd, check=False)\n",
        "    return True\n",
        "\n",
        "workers = 4\n",
        "with ProcessPoolExecutor(max_workers=workers) as exe:\n",
        "    futures = [exe.submit(convert_one, p) for p in to_do]\n",
        "    with tqdm(total=total, ncols=80, desc=\"Конвертация\") as pbar:\n",
        "        for _ in as_completed(futures):\n",
        "            pbar.update(1)\n",
        "\n",
        "print(\" Все файлы сконвертированы!\")\n"
      ],
      "metadata": {
        "id": "ePyyzhOiknIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchaudio.transforms import Resample, MelSpectrogram, AmplitudeToDB\n",
        "\n",
        "csv_path = \"/content/drive/My Drive/df_IEMOCAP_with_text.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "audio_dir = \"/content/audio\""
      ],
      "metadata": {
        "id": "NYdsxGqwmscv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = []\n",
        "labels     = []\n",
        "label_set  = sorted(df['emotion'].unique())\n",
        "label_map  = {emo: i for i, emo in enumerate(label_set)}\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    name    = row['name_files']\n",
        "    session = row['session']\n",
        "    emo     = row['emotion']\n",
        "    wav_fn  = f\"{name}.wav\"\n",
        "    wav_fp  = os.path.join(audio_dir, wav_fn)\n",
        "    if os.path.isfile(wav_fp):\n",
        "        file_paths.append(wav_fp)\n",
        "        labels.append(label_map[emo])\n",
        "\n",
        "print(f\"Используем {len(file_paths)} файлов, метки: {label_map}\")"
      ],
      "metadata": {
        "id": "UwIN54uKoKnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, os\n",
        "from pathlib import Path\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/df_IEMOCAP_with_text.csv\")\n",
        "\n",
        "target_emotions = {\"ang\",\"dis\",\"fea\",\"hap\",\"sad\",\"sur\", \"neu\"}\n",
        "\n",
        "df = df[df['emotion'].isin(target_emotions)].reset_index(drop=True)\n",
        "\n",
        "label_map = {emo: idx for idx, emo in enumerate(sorted(target_emotions))}\n",
        "print(\"Новые метки:\", label_map)\n",
        "\n",
        "audio_dir = \"/content/audio\"\n",
        "file_paths, labels = [], []\n",
        "for _, row in df.iterrows():\n",
        "    wav_fn = f\"{row['name_files']}.wav\"\n",
        "    wav_fp = os.path.join(audio_dir, wav_fn)\n",
        "    if os.path.isfile(wav_fp):\n",
        "        file_paths.append(wav_fp)\n",
        "        labels.append(label_map[row['emotion']])\n",
        "\n",
        "print(f\"Осталось для обучения {len(file_paths)} файлов в {len(label_map)} классах\")"
      ],
      "metadata": {
        "id": "hs76Cu72o-uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "cnt = Counter(labels)\n",
        "print(\"До фильтрации:\", cnt)\n",
        "\n",
        "min_samples = 2\n",
        "good_classes = {lab for lab, c in cnt.items() if c >= min_samples}\n",
        "print(\"Оставляем классы:\", good_classes)\n",
        "\n",
        "file_paths_f = []\n",
        "labels_f     = []\n",
        "\n",
        "for p, l in zip(file_paths, labels):\n",
        "    if l in good_classes:\n",
        "        file_paths_f.append(p)\n",
        "        labels_f.append(l)\n",
        "\n",
        "cnt2 = Counter(labels_f)\n",
        "print(\"После фильтрации:\", cnt2)\n",
        "\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    file_paths_f, labels_f,\n",
        "    test_size=0.2,\n",
        "    stratify=labels_f,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train:\", len(train_paths), \"Val:\", len(val_paths))\n"
      ],
      "metadata": {
        "id": "fdd0lKOlpUrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchaudio.transforms import Resample, MelSpectrogram, AmplitudeToDB\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import librosa\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def extract_prosody(wav_fp, sr=16000):\n",
        "    y, _ = librosa.load(wav_fp, sr=sr)\n",
        "    pitches, _ = librosa.piptrack(y=y, sr=sr)\n",
        "    pitch = float(np.mean(pitches[pitches>0]) or 0.0)\n",
        "    energy = float(np.mean(librosa.feature.rms(y=y)))\n",
        "    return np.array([pitch, energy], dtype=np.float32)\n",
        "\n",
        "class EmotionDatasetPlus(torch.utils.data.Dataset):\n",
        "    def __init__(self, paths, labels, sr=16000, n_mels=64,\n",
        "                 mask_prob=0.3, mask_patch_size=(10,10)):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.mask_prob = mask_prob\n",
        "        self.patch = mask_patch_size\n",
        "        self.resampler = Resample(orig_freq=48000, new_freq=sr)\n",
        "        self.melspec   = MelSpectrogram(sample_rate=sr, n_mels=n_mels)\n",
        "        self.db        = AmplitudeToDB()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        wav_fp = self.paths[i]\n",
        "        wav, sr = torchaudio.load(wav_fp)\n",
        "        if sr!=16000: wav = self.resampler(wav)\n",
        "        if wav.size(0)>1: wav = wav.mean(0,keepdim=True)\n",
        "        mel = self.melspec(wav)\n",
        "        mel_db = self.db(mel).squeeze(0)\n",
        "\n",
        "        M, T = mel_db.shape\n",
        "        mask = torch.ones_like(mel_db)\n",
        "        n_patches = int(self.mask_prob * (M*T)/(self.patch[0]*self.patch[1]))\n",
        "        for _ in range(n_patches):\n",
        "            t0 = np.random.randint(0, T-self.patch[0])\n",
        "            m0 = np.random.randint(0, M-self.patch[1])\n",
        "            mask[m0:m0+self.patch[1], t0:t0+self.patch[0]] = 0\n",
        "        mel_db = mel_db * mask\n",
        "\n",
        "        pros = torch.from_numpy(extract_prosody(wav_fp)).float()\n",
        "        return mel_db, pros, self.labels[i]\n",
        "\n",
        "def collate_fn_plus(batch):\n",
        "    specs, prosody, labs = zip(*batch)\n",
        "    max_t = max(s.shape[-1] for s in specs)\n",
        "    specs_padded = [torch.nn.functional.pad(s, (0, max_t-s.shape[-1])) for s in specs]\n",
        "    return torch.stack(specs_padded), torch.stack([torch.tensor(p) for p in prosody]), torch.tensor(labs)\n",
        "\n",
        "\n",
        "train_ds = EmotionDatasetPlus(train_paths, train_labels, mask_prob=0.3)\n",
        "val_ds   = EmotionDatasetPlus(val_paths,   val_labels,   mask_prob=0.3)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_fn_plus\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_fn_plus\n",
        ")\n",
        "\n",
        "batch_mel, batch_pros, batch_lbl = next(iter(train_loader))\n",
        "print(\"mel:\", batch_mel.shape, \"prosody:\", batch_pros.shape, \"labels:\", batch_lbl.shape)\n"
      ],
      "metadata": {
        "id": "KwwDm49ypwub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import AutoModel\n",
        "\n",
        "backbone = AutoModel.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "backbone = get_peft_model(backbone, lora_cfg)\n",
        "\n",
        "class EmotionModel(nn.Module):\n",
        "    def __init__(self, backbone, n_classes, prosody_dim=2):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        H = backbone.config.hidden_size\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(H + prosody_dim, H),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(H, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, mel, pros):\n",
        "        out  = self.backbone(mel).last_hidden_state\n",
        "        feat = out.mean(dim=1)\n",
        "        x    = torch.cat([feat, pros], dim=1)\n",
        "        return self.head(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EmotionModel(backbone, n_classes=6, prosody_dim=2).to(device)"
      ],
      "metadata": {
        "id": "aPM4mZY5smd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for mel, pros, y in loader:\n",
        "            mel, pros, y = mel.to(device), pros.to(device), y.to(device)\n",
        "            logits = model(mel, pros)\n",
        "            p = logits.argmax(dim=1)\n",
        "            preds += p.cpu().tolist()\n",
        "            trues += y.cpu().tolist()\n",
        "    return accuracy_score(trues, preds), f1_score(trues, preds, average=\"macro\")\n",
        "\n",
        "epochs = 5\n",
        "for ep in range(1, epochs+1):\n",
        "    p = 0.3 + 0.4*(ep-1)/(epochs-1)\n",
        "    train_ds.mask_prob = p\n",
        "    val_ds.mask_prob   = p\n",
        "\n",
        "    model.train()\n",
        "    loop = tqdm(train_loader, desc=f\"Train Ep{ep} (mask={p:.2f})\")\n",
        "    for mel, pros, y in loop:\n",
        "        mel, pros, y = mel.to(device), pros.to(device), y.to(device)\n",
        "        logits = model(mel, pros)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        loop.set_postfix(loss=f\"{loss.item():.3f}\")\n",
        "\n",
        "    acc, f1 = evaluate(val_loader)\n",
        "    print(f\"→ Ep{ep}: Val Acc={acc:.3f}, Macro-F1={f1:.3f}\\n\")\n"
      ],
      "metadata": {
        "id": "_vEhgY35uu4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r audio.zip /content/audio\n",
        "\n"
      ],
      "metadata": {
        "id": "X864ENmDv7Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"audio.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "eEOeUTUG4qgT",
        "outputId": "f7f8d601-ce65-491c-de97-7412ead87d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c8b50396-b326-4079-8b12-e6ff31d41796\", \"audio.zip\", 1022389987)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}