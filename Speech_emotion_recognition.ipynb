{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/audio.zip\" -d /content/audio"
      ],
      "metadata": {
        "id": "_SyprlaM77E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_dir = \"/content/audio/content/audio\"\n",
        "csv_path  = \"/content/df_IEMOCAP_with_text.csv\""
      ],
      "metadata": {
        "id": "SyzsTRtW8vKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "target = {\"ang\",\"dis\",\"fea\",\"hap\",\"sad\",\"sur\", \"neu\"}\n",
        "df = df[df['emotion'].isin(target)].reset_index(drop=True)\n",
        "label_map = {emo:i for i, emo in enumerate(sorted(target))}\n",
        "\n",
        "file_paths, labels = [], []\n",
        "for _, row in df.iterrows():\n",
        "    wav_fn = f\"{row['name_files']}.wav\"\n",
        "    wav_fp = os.path.join(audio_dir, wav_fn)\n",
        "    if os.path.isfile(wav_fp):\n",
        "        file_paths.append(wav_fp)\n",
        "        labels.append(label_map[row['emotion']])\n",
        "\n",
        "print(f\"Найдено {len(file_paths)} файлов для обучения\")"
      ],
      "metadata": {
        "id": "RzymAaCv8zCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    file_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "print(\"Train/Val:\", len(train_paths), \"/\", len(val_paths))"
      ],
      "metadata": {
        "id": "975ZoYsx9iOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchaudio.transforms import Resample, MelSpectrogram, AmplitudeToDB\n",
        "\n",
        "class SimpleAudioDataset(Dataset):\n",
        "    def __init__(self, paths, labels, sr=16000, n_mels=64):\n",
        "        self.paths, self.labels = paths, labels\n",
        "        self.resampler = Resample(orig_freq=48000, new_freq=sr)\n",
        "        self.melspec   = MelSpectrogram(sample_rate=sr, n_mels=n_mels)\n",
        "        self.db        = AmplitudeToDB()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        path = self.paths[i]\n",
        "        try:\n",
        "            wav, sr = torchaudio.load(path)\n",
        "        except Exception:\n",
        "            return self.__getitem__((i+1) % len(self))\n",
        "        if sr != 16000:\n",
        "            wav = self.resampler(wav)\n",
        "        if wav.size(0) > 1:\n",
        "            wav = wav.mean(0, keepdim=True)\n",
        "        mel_db = self.db(self.melspec(wav)).squeeze(0)\n",
        "        label  = self.labels[i]\n",
        "        return mel_db, label\n",
        "\n",
        "def collate_fn(batch):\n",
        "    specs, labs = zip(*batch)\n",
        "    max_t = max(s.shape[-1] for s in specs)\n",
        "    padded = [torch.nn.functional.pad(s, (0, max_t-s.shape[-1])) for s in specs]\n",
        "    return torch.stack(padded), torch.tensor(labs)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    SimpleAudioDataset(train_paths, train_labels),\n",
        "    batch_size=16, shuffle=True,  num_workers=0, collate_fn=collate_fn\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    SimpleAudioDataset(val_paths,   val_labels),\n",
        "    batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "4FGZCK8Z9tdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_mel, batch_lbl = next(iter(train_loader))\n",
        "print(\"Batch mel:\", batch_mel.shape, \"Labels:\", batch_lbl.shape)"
      ],
      "metadata": {
        "id": "nG2XsOii9x4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MelCNN(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(32, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        feat = self.features(x)\n",
        "        return self.classifier(feat)\n"
      ],
      "metadata": {
        "id": "lzYGYlpN-Opm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MelCNN(n_classes=7).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "JHpZVZ1b_Xz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for mel, y in loader:\n",
        "            mel, y = mel.to(device), y.to(device)\n",
        "            logits = model(mel)\n",
        "            p = logits.argmax(dim=1)\n",
        "            preds += p.cpu().tolist()\n",
        "            trues += y.cpu().tolist()\n",
        "    return accuracy_score(trues, preds), f1_score(trues, preds, average=\"macro\")\n",
        "\n",
        "epochs = 5\n",
        "for ep in range(1, epochs+1):\n",
        "    model.train()\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {ep}\")\n",
        "    for X, y in loop:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        logits = model(X)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        loop.set_postfix(loss=f\"{loss.item():.3f}\")\n",
        "    acc, f1 = evaluate(val_loader)\n",
        "    print(f\"→ Ep{ep}: Val Acc={acc:.3f}, Macro-F1={f1:.3f}\")"
      ],
      "metadata": {
        "id": "7aoPwX0r-Ykg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "CSV_ALL   = \"/content/drive/MyDrive/df_IEMOCAP_with_text.csv\"\n",
        "AUDIO_DIR = \"/content/drive/MyDrive/iemocap_audio\"\n",
        "\n",
        "df = pd.read_csv(CSV_ALL)\n",
        "\n",
        "target = [\"ang\", \"dis\", \"fea\", \"hap\", \"sad\", \"sur\"]\n",
        "df = df[df[\"emotion\"].isin(target)].reset_index(drop=True)\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df[\"emotion\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_df[\"path_to_wav\"] = train_df[\"name_files\"].apply(\n",
        "    lambda fn: os.path.join(AUDIO_DIR, fn + \".wav\")\n",
        ")\n",
        "val_df[\"path_to_wav\"] = val_df[\"name_files\"].apply(\n",
        "    lambda fn: os.path.join(AUDIO_DIR, fn + \".wav\")\n",
        ")\n",
        "\n",
        "for split_df, name in [(train_df, \"train\"), (val_df, \"validation\")]:\n",
        "    missing = split_df[\"path_to_wav\"].apply(os.path.exists).value_counts()\n",
        "    print(f\"{name} — файлы найдены? \\n{missing}\\n\")\n",
        "\n",
        "ds = DatasetDict({\n",
        "    \"train\":      Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
        "    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
        "})\n",
        "\n",
        "print(ds)\n",
        "print(\"Пример записи:\", ds[\"train\"][0])\n"
      ],
      "metadata": {
        "id": "sf7fQx5_AceL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df[train_df[\"path_to_wav\"].apply(os.path.exists)].reset_index(drop=True)\n",
        "val_df   = val_df[  val_df[\"path_to_wav\"].apply(os.path.exists)].reset_index(drop=True)\n",
        "\n",
        "ds = DatasetDict({\n",
        "    \"train\":      Dataset.from_pandas(train_df),\n",
        "    \"validation\": Dataset.from_pandas(val_df),\n",
        "})"
      ],
      "metadata": {
        "id": "CAVCeYSLBkum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import librosa, os\n",
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "\n",
        "def is_loadable(path):\n",
        "    try:\n",
        "        _y, _ = librosa.load(path, sr=16000)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "tqdm.pandas()\n",
        "print(\"Train before:\", len(train_df))\n",
        "train_df[\"ok\"] = train_df[\"path_to_wav\"].progress_apply(is_loadable)\n",
        "train_df = train_df[train_df[\"ok\"]].drop(columns=\"ok\").reset_index(drop=True)\n",
        "print(\"Train after :\", len(train_df))\n",
        "\n",
        "print(\"Valid before:\", len(val_df))\n",
        "val_df[\"ok\"] = val_df[\"path_to_wav\"].progress_apply(is_loadable)\n",
        "val_df = val_df[val_df[\"ok\"]].drop(columns=\"ok\").reset_index(drop=True)\n",
        "print(\"Valid after :\", len(val_df))\n",
        "\n",
        "ds = DatasetDict({\n",
        "    \"train\":      Dataset.from_pandas(train_df),\n",
        "    \"validation\": Dataset.from_pandas(val_df),\n",
        "})\n",
        "\n",
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "fe = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "label_list = sorted(ds[\"train\"].unique(\"emotion\"))\n",
        "label2id    = {e:i for i,e in enumerate(label_list)}\n",
        "\n",
        "def preprocess(batch):\n",
        "    speech, sr = librosa.load(batch[\"path_to_wav\"], sr=16000)\n",
        "    inputs = fe(speech, sampling_rate=sr, return_tensors=\"pt\")\n",
        "    return {\n",
        "        \"input_values\": inputs.input_values[0].numpy(),\n",
        "        \"labels\":       label2id[batch[\"emotion\"]]\n",
        "    }\n",
        "\n",
        "ds = ds.map(\n",
        "    preprocess,\n",
        "    remove_columns=ds[\"train\"].column_names,\n",
        "    batched=False,\n",
        "    num_proc=4,\n",
        "    load_from_cache_file=False\n",
        ")"
      ],
      "metadata": {
        "id": "UV0r3FMWwKsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import (\n",
        "    Wav2Vec2ForSequenceClassification,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    TrainerCallback,\n",
        "    TrainerState,\n",
        "    TrainerControl\n",
        ")\n",
        "import evaluate\n",
        "\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    num_labels=len(label2id),\n",
        "    label2id=label2id,\n",
        "    id2label={i: e for e, i in label2id.items()},\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits = pred.predictions\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=preds, references=pred.label_ids, average=\"macro\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"wav2vec2_ser\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "class ConsoleProgressCallback(TrainerCallback):\n",
        "    def on_step_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        if not state.log_history:\n",
        "            return\n",
        "        logs = state.log_history[-1]\n",
        "        if \"loss\" in logs and state.global_step % args.logging_steps == 0:\n",
        "            print(f\"[Step {state.global_step}/{state.max_steps}]  Loss: {logs['loss']:.4f}\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds[\"train\"],\n",
        "    eval_dataset= ds[\"validation\"],\n",
        "    tokenizer=fe,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[ConsoleProgressCallback]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "dKWMKLM1I4Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "from transformers import (\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2ForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    TrainerCallback, TrainerState, TrainerControl,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "WBO3k3FLTpoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_ALL   = \"/content/df_IEMOCAP_with_text.csv\"\n",
        "AUDIO_DIR = \"/content/audio/content/audio\"\n",
        "\n",
        "df = pd.read_csv(CSV_ALL)\n",
        "target = [\"ang\", \"dis\", \"fea\", \"hap\", \"sad\", \"sur\"]\n",
        "df = df[df[\"emotion\"].isin(target)].reset_index(drop=True)\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    df, test_size=0.2, stratify=df[\"emotion\"], random_state=42\n",
        ")\n",
        "train_df[\"path_to_wav\"] = train_df[\"name_files\"].map(lambda fn: f\"{AUDIO_DIR}/{fn}.wav\")\n",
        "val_df  [\"path_to_wav\"] = val_df  [\"name_files\"].map(lambda fn: f\"{AUDIO_DIR}/{fn}.wav\")\n",
        "\n",
        "print(\"Train files:\", train_df[\"path_to_wav\"].map(os.path.exists).value_counts())\n",
        "print(\"Val   files:\", val_df  [\"path_to_wav\"].map(os.path.exists).value_counts())"
      ],
      "metadata": {
        "id": "bWIQp2zdTscl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f13272c-756a-4ecc-a859-13b3f5795117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train files: path_to_wav\n",
            "True     2343\n",
            "False       1\n",
            "Name: count, dtype: int64\n",
            "Val   files: path_to_wav\n",
            "True     586\n",
            "False      1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = DatasetDict({\n",
        "    \"train\":      Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
        "    \"validation\": Dataset.from_pandas(val_df .reset_index(drop=True)),\n",
        "})\n",
        "\n",
        "def is_valid(ex):\n",
        "    try:\n",
        "        sf.info(ex[\"path_to_wav\"])\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "ds = ds.filter(is_valid)"
      ],
      "metadata": {
        "id": "ljnSmR-2TvTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fe = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "label_list = sorted(ds[\"train\"].unique(\"emotion\"))\n",
        "label2id    = {e:i for i,e in enumerate(label_list)}\n",
        "id2label    = {i:e for e,i in label2id.items()}"
      ],
      "metadata": {
        "id": "QXKNUHBxTxqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(example):\n",
        "    speech, sr = librosa.load(example[\"path_to_wav\"], sr=16_000)\n",
        "    inputs = fe(speech, sampling_rate=sr, return_tensors=\"np\")  # numpy\n",
        "    rms = librosa.feature.rms(y=speech)[0]\n",
        "    prosody = np.array([rms.mean(), rms.std()], dtype=np.float32)\n",
        "\n",
        "    return {\n",
        "        \"input_values\": inputs[\"input_values\"][0],\n",
        "        \"labels\":       label2id[example[\"emotion\"]],\n",
        "        \"prosody\":      prosody,\n",
        "    }\n",
        "\n",
        "ds = ds.map(\n",
        "    preprocess,\n",
        "    remove_columns=ds[\"train\"].column_names,\n",
        "    batched=False\n",
        ")\n",
        "\n",
        "print(\"После map:\", len(ds[\"train\"]), \"записей; пример:\", ds[\"train\"][0])"
      ],
      "metadata": {
        "id": "nehjIE2xT0Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(\n",
        "    tokenizer=fe,\n",
        "    padding=True\n",
        ")"
      ],
      "metadata": {
        "id": "iW_Eh4qHEMbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    num_labels=len(label2id),\n",
        "    label2id=label2id,\n",
        "    id2label={i: e for e, i in label2id.items()},\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"f1\")\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return metric.compute(predictions=preds, references=p.label_ids, average=\"macro\")"
      ],
      "metadata": {
        "id": "cSmrZWDnEOww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")"
      ],
      "metadata": {
        "id": "DygZW79kxtTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "class ProsodyCollator:\n",
        "    def __init__(self, processor):\n",
        "        self.base_collator = DataCollatorWithPadding(tokenizer=processor)\n",
        "\n",
        "    def __call__(self, features):\n",
        "        prosody_features = [f.pop(\"prosody\") for f in features]\n",
        "        batch = self.base_collator(features)\n",
        "\n",
        "\n",
        "\n",
        "        return batch\n",
        "\n",
        "prosody_data_collator = ProsodyCollator(processor=processor)"
      ],
      "metadata": {
        "id": "-idbB3-9xLkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ProgressCallback\n",
        "\n",
        "class ConsoleProgress(TrainerCallback):\n",
        "    def on_step_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        if state.global_step % args.logging_steps == 0 and state.log_history:\n",
        "            loss = state.log_history[-1].get(\"loss\")\n",
        "            if loss is not None:\n",
        "                print(f\"[Step {state.global_step}/{state.max_steps}]  Loss: {loss:.4f}\")\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./wav2vec2_with_prosody\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=50,\n",
        "    eval_steps=200,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds[\"train\"],\n",
        "    eval_dataset=ds[\"validation\"],\n",
        "    data_collator=prosody_data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[ConsoleProgress, ProgressCallback],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "s4fjYF1IETcd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}